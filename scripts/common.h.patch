--- common.h.orig	2024-11-19 11:11:05
+++ common.h	2024-11-19 11:07:10
@@ -40,6 +40,17 @@
 extern char const * LLAMA_BUILD_TARGET;

 struct common_control_vector_load_info;
+
+#define print_build_info() do {                                                                     \
+    fprintf(stderr, "%s: build = %d (%s)\n", __func__, LLAMA_BUILD_NUMBER, LLAMA_COMMIT);           \
+    fprintf(stderr, "%s: built with %s for %s\n", __func__, LLAMA_COMPILER, LLAMA_BUILD_TARGET);    \
+} while(0)
+
+// build info
+extern int LLAMA_BUILD_NUMBER;
+extern char const *LLAMA_COMMIT;
+extern char const *LLAMA_COMPILER;
+extern char const *LLAMA_BUILD_TARGET;

 //
 // CPU utils
@@ -154,6 +165,7 @@
 };

 struct common_params {
+    bool vocab_only               = false;
     int32_t n_predict             =    -1; // new tokens to predict
     int32_t n_ctx                 =  4096; // context size
     int32_t n_batch               =  2048; // logical batch size for prompt processing (must be >=32 to use BLAS)
@@ -269,6 +281,9 @@
     bool no_kv_offload     = false; // disable KV offloading
     bool warmup            = true;  // warmup run
     bool check_tensors     = false; // validate tensor data
+
+    llama_progress_callback progress_callback;
+    void * progress_callback_user_data;

     std::string cache_type_k = "f16"; // KV cache data type for the K
     std::string cache_type_v = "f16"; // KV cache data type for the V
@@ -461,6 +476,9 @@
 // clear LoRA adapters from context, then apply new list of adapters
 void common_lora_adapters_apply(struct llama_context * ctx, std::vector<common_lora_adapter_container> & lora_adapters);

+// remove LoRA adapters from context
+void common_lora_adapters_remove(struct llama_context * ctx, std::vector<common_lora_adapter_container> & lora_adapters);
+
 // Batch utils

 void common_batch_clear(struct llama_batch & batch);
